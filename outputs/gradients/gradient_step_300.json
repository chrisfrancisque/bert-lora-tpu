{
  "step": 300,
  "lora_gradients": {
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight": 0.0038909912109375,
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight": 0.020263671875,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight": 0.0174560546875,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight": 0.07763671875,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight": 0.006988525390625,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight": 0.025390625,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight": 0.01397705078125,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight": 0.11865234375,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight": 0.0026092529296875,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight": 0.0159912109375,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight": 0.00897216796875,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight": 0.1181640625,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight": 0.0033416748046875,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight": 0.018310546875,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight": 0.0194091796875,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight": 0.185546875,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight": 0.0035400390625,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight": 0.01806640625,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight": 0.0260009765625,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight": 0.1796875,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight": 0.00421142578125,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight": 0.0283203125,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight": 0.01904296875,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight": 0.1875,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight": 0.0031890869140625,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight": 0.0205078125,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight": 0.0302734375,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight": 0.21484375,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight": 0.003326416015625,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight": 0.017822265625,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight": 0.0269775390625,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight": 0.2236328125,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight": 0.0084228515625,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight": 0.036376953125,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight": 0.048828125,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight": 0.3359375,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight": 0.016357421875,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight": 0.038818359375,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight": 0.1103515625,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight": 0.224609375,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight": 0.014892578125,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight": 0.06591796875,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight": 0.353515625,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight": 0.19140625,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight": 0.01214599609375,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight": 0.0286865234375,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight": 0.20703125,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight": 0.09228515625
  },
  "base_gradients": {
    "base_model.model.classifier.modules_to_save.default.weight": 0.6015625,
    "base_model.model.classifier.modules_to_save.default.bias": 0.027099609375
  },
  "statistics": {
    "lora_mean": 0.07185681660970052,
    "lora_max": 0.353515625,
    "lora_min": 0.0026092529296875,
    "base_mean": 0.3143310546875,
    "base_max": 0.6015625
  }
}