{
  "step": 200,
  "lora_gradients": {
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight": 0.00146484375,
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight": 0.00885009765625,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight": 0.005096435546875,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight": 0.0279541015625,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight": 0.00151824951171875,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight": 0.00946044921875,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight": 0.0026702880859375,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight": 0.0380859375,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight": 0.00119781494140625,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight": 0.0084228515625,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight": 0.00286865234375,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight": 0.042236328125,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight": 0.00131988525390625,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight": 0.0098876953125,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight": 0.00848388671875,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight": 0.05419921875,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight": 0.0018463134765625,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight": 0.01361083984375,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight": 0.0074462890625,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight": 0.04541015625,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight": 0.00150299072265625,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight": 0.01348876953125,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight": 0.0091552734375,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight": 0.0498046875,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight": 0.001251220703125,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight": 0.00933837890625,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight": 0.0174560546875,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight": 0.06494140625,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight": 0.00133514404296875,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight": 0.0067138671875,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight": 0.0169677734375,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight": 0.068359375,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight": 0.00286865234375,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight": 0.015380859375,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight": 0.061279296875,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight": 0.1181640625,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight": 0.0034942626953125,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight": 0.013427734375,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight": 0.04150390625,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight": 0.080078125,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight": 0.003753662109375,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight": 0.022216796875,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight": 0.06591796875,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight": 0.0498046875,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight": 0.00421142578125,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight": 0.01220703125,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight": 0.05810546875,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight": 0.0301513671875
  },
  "base_gradients": {
    "base_model.model.classifier.modules_to_save.default.weight": 0.97265625,
    "base_model.model.classifier.modules_to_save.default.bias": 0.04052734375
  },
  "statistics": {
    "lora_mean": 0.023643970489501953,
    "lora_max": 0.1181640625,
    "lora_min": 0.00119781494140625,
    "base_mean": 0.506591796875,
    "base_max": 0.97265625
  }
}