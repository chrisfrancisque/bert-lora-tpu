{
  "step": 100,
  "lora_gradients": {
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_A.default.weight": 0.0017242431640625,
    "base_model.model.bert.encoder.layer.0.attention.self.query.lora_B.default.weight": 0.006561279296875,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_A.default.weight": 0.0089111328125,
    "base_model.model.bert.encoder.layer.0.attention.self.value.lora_B.default.weight": 0.0274658203125,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_A.default.weight": 0.00054168701171875,
    "base_model.model.bert.encoder.layer.1.attention.self.query.lora_B.default.weight": 0.00543212890625,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_A.default.weight": 0.004669189453125,
    "base_model.model.bert.encoder.layer.1.attention.self.value.lora_B.default.weight": 0.0245361328125,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_A.default.weight": 0.0003147125244140625,
    "base_model.model.bert.encoder.layer.2.attention.self.query.lora_B.default.weight": 0.003631591796875,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_A.default.weight": 0.006866455078125,
    "base_model.model.bert.encoder.layer.2.attention.self.value.lora_B.default.weight": 0.024658203125,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_A.default.weight": 0.0002651214599609375,
    "base_model.model.bert.encoder.layer.3.attention.self.query.lora_B.default.weight": 0.004486083984375,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_A.default.weight": 0.006103515625,
    "base_model.model.bert.encoder.layer.3.attention.self.value.lora_B.default.weight": 0.0252685546875,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_A.default.weight": 0.00193023681640625,
    "base_model.model.bert.encoder.layer.4.attention.self.query.lora_B.default.weight": 0.00927734375,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_A.default.weight": 0.0057373046875,
    "base_model.model.bert.encoder.layer.4.attention.self.value.lora_B.default.weight": 0.0216064453125,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_A.default.weight": 0.00060272216796875,
    "base_model.model.bert.encoder.layer.5.attention.self.query.lora_B.default.weight": 0.006134033203125,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_A.default.weight": 0.00665283203125,
    "base_model.model.bert.encoder.layer.5.attention.self.value.lora_B.default.weight": 0.021484375,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_A.default.weight": 0.00054168701171875,
    "base_model.model.bert.encoder.layer.6.attention.self.query.lora_B.default.weight": 0.005859375,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_A.default.weight": 0.009033203125,
    "base_model.model.bert.encoder.layer.6.attention.self.value.lora_B.default.weight": 0.0267333984375,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_A.default.weight": 0.0002899169921875,
    "base_model.model.bert.encoder.layer.7.attention.self.query.lora_B.default.weight": 0.00311279296875,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_A.default.weight": 0.0096435546875,
    "base_model.model.bert.encoder.layer.7.attention.self.value.lora_B.default.weight": 0.0250244140625,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_A.default.weight": 0.00099945068359375,
    "base_model.model.bert.encoder.layer.8.attention.self.query.lora_B.default.weight": 0.007354736328125,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_A.default.weight": 0.01385498046875,
    "base_model.model.bert.encoder.layer.8.attention.self.value.lora_B.default.weight": 0.0286865234375,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_A.default.weight": 0.00156402587890625,
    "base_model.model.bert.encoder.layer.9.attention.self.query.lora_B.default.weight": 0.006500244140625,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_A.default.weight": 0.0108642578125,
    "base_model.model.bert.encoder.layer.9.attention.self.value.lora_B.default.weight": 0.0233154296875,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_A.default.weight": 0.00185394287109375,
    "base_model.model.bert.encoder.layer.10.attention.self.query.lora_B.default.weight": 0.0125732421875,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_A.default.weight": 0.01348876953125,
    "base_model.model.bert.encoder.layer.10.attention.self.value.lora_B.default.weight": 0.0233154296875,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_A.default.weight": 0.00103759765625,
    "base_model.model.bert.encoder.layer.11.attention.self.query.lora_B.default.weight": 0.005828857421875,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_A.default.weight": 0.032958984375,
    "base_model.model.bert.encoder.layer.11.attention.self.value.lora_B.default.weight": 0.035400390625
  },
  "base_gradients": {
    "base_model.model.classifier.modules_to_save.default.weight": 0.99609375,
    "base_model.model.classifier.modules_to_save.default.bias": 0.06884765625
  },
  "statistics": {
    "lora_mean": 0.010931173960367838,
    "lora_max": 0.035400390625,
    "lora_min": 0.0002651214599609375,
    "base_mean": 0.532470703125,
    "base_max": 0.99609375
  }
}